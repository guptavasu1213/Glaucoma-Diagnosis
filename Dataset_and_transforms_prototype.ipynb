{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset and transforms prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptavasu1213/glaucoma-identification/blob/master/Dataset_and_transforms_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQ3cjRjkzx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1aaaed10-edd4-4a98-876f-db089dbd97bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXdcBV36OufE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell allows us to import local modules from google drive\n",
        "import sys\n",
        "sys.path.append('/content/drive/Shared drives/Capstone Summer 2020/Collab Files/Scripts')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttLY7iO2xwzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBIy-MRixuJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = 'drive/Shared drives/Capstone Summer 2020/Data/Training/'\n",
        "test_path = 'drive/Shared drives/Capstone Summer 2020/Data/Testing/'\n",
        "#https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#torchvision-object-detection-finetuning-tutorial\n",
        "\n",
        "class Dataset(BaseDataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.images_dir = os.path.join(root_dir, 'Images')\n",
        "    self.masks_dir =os.path.join(root_dir, 'Masks')\n",
        "    self.transform = transform\n",
        "    self.ids = os.listdir(self.images_dir)\n",
        "    self.maskids = os.listdir(self.masks_dir)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #load images and masks\n",
        "    img_path = os.path.join(self.images_dir, self.ids[idx])\n",
        "    mask_path = os.path.join(self.masks_dir, self.maskids[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path)\n",
        "\n",
        "    sample = {'image': img, 'mask': mask}\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrWsef3bZBMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.transforms import functional as F\n",
        "\n",
        "class Resize(object):\n",
        "  def __init__(self, img_resize):\n",
        "    self.img_resize = img_resize\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    image, mask = sample['image'], sample['mask']\n",
        "    mask = mask.resize(self.img_resize)\n",
        "    image = image.resize(self.img_resize)\n",
        "    return {'image': image, 'mask': mask}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    image = F.to_tensor(sample['image'])\n",
        "    mask = F.to_tensor(sample['mask'])\n",
        "    return {'image': image, 'mask': mask}\n",
        "\n",
        "\n",
        "class ToPILImage(object):\n",
        "  def __call__(self, sample):\n",
        "    image = F.to_pil_image(sample['image'])\n",
        "    mask = F.to_pil_image(sample['mask'])\n",
        "    return {'image': image, 'mask': mask}\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "  def __init__(self, mean, std):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    # we don't need to nromailze masks\n",
        "    image, mask = sample['image'], mask['mask']\n",
        "    image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "    return {'image': image, 'mask': mask}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TetG7pcZC3Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_transform():\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor()) \n",
        "    return transforms.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIuAoyYK7Ezr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import copy\n",
        "import time\n",
        "from tqdm import tqdm \n",
        " \n",
        "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath, num_epochs=3):\n",
        "    since = time.time()\n",
        "    #Extracting the weights and biases from the model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    # Use gpu if available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # Initialize the log file for training and testing loss and metrics\n",
        "    # fieldnames = ['epoch', 'Train_loss', 'Test_loss'] + \\\n",
        "    #     [f'Train_{m}' for m in metrics.keys()] + \\\n",
        "    #     [f'Test_{m}' for m in metrics.keys()]\n",
        "    # with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
        "    #     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    #     writer.writeheader()\n",
        " \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        # Initialize batch summary\n",
        "        # batchsummary = {a: [0] for a in fieldnames}\n",
        " \n",
        "        for phase in ['Train', 'Test']:\n",
        "            if phase == 'Train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        " \n",
        "            # Iterate over data.\n",
        "            for sample in tqdm(iter(dataloaders[phase])):\n",
        "                inputs = sample['image'].to(device)\n",
        "                masks = sample['mask'].to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        " \n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "                    #Train                    \n",
        "                    outputs = model(inputs)\n",
        "                    #Compute loss\n",
        "                    loss = criterion(outputs['out'], masks)\n",
        "                    \n",
        "                    #Logging the results\n",
        "                    # y_pred = outputs['out'].data.cpu().numpy().ravel()\n",
        "                    # y_true = masks.data.cpu().numpy().ravel()\n",
        "                    # for name, metric in metrics.items():\n",
        "                    #     if name == 'f1_score':\n",
        "                    #         # Use a classification threshold of 0.1\n",
        "                    #         batchsummary[f'{phase}_{name}'].append(\n",
        "                    #             metric(y_true > 0, y_pred > 0.1))\n",
        "                    #     else:\n",
        "                    #         batchsummary[f'{phase}_{name}'].append(\n",
        "                    #             metric(y_true.astype('uint8'), y_pred))\n",
        " \n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'Train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "            # batchsummary['epoch'] = epoch\n",
        "            epoch_loss = loss\n",
        "            # batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, loss))\n",
        "        # for field in fieldnames[3:]:\n",
        "        #     batchsummary[field] = np.mean(batchsummary[field])\n",
        "        # print(batchsummary)\n",
        "        # with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
        "        #     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        #     writer.writerow(batchsummary)\n",
        "        #     # deep copy the model\n",
        "        if phase == 'Test' and loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        " \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
        " \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uQovV1S5O5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image():\n",
        "  return transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1YnulfrWP64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(img, model):\n",
        "  #Preprocess the raw image\n",
        "  transform = preprocess_image()\n",
        "  img = transform(img)\n",
        "\n",
        "  # Use gpu if available\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  img = img.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "  # img = img.view(1, 3, 512, 512)\n",
        "  img = img.type(torch.cuda.FloatTensor)\n",
        "  \n",
        "  #Run prediction\n",
        "  output = model(img)\n",
        "  mask = output['out'][0]\n",
        "  print(mask.shape)\n",
        "  return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHjf351UGjeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model():\n",
        "  model = torch.load('drive/Shared drives/Capstone Summer 2020/Models/weights.pt')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le5Xa77v0Dwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([Resize((512, 512)), ToTensor()])\n",
        "# Creating test and train datasets\n",
        "train_dataset = Dataset(train_path, transform=transform)\n",
        "test_dataset = Dataset(test_path, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLUjgMtPPqEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating test and train dataloaders\n",
        "train_dataloader =  DataLoader (train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
        "test_dataloader =  DataLoader (test_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
        "dataloaders = {'Train': train_dataloader, 'Test': test_dataloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Viwcpc5rN8tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from deeplabv3 import DeepLabHead\n",
        "\n",
        "#Model initialization\n",
        "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
        "\n",
        "# Added a Sigmoid activation after the last convolution layer\n",
        "model.classifier = DeepLabHead(2048, 1) # Specifying the output classes we need\n",
        "\n",
        "# Specify the loss function\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "# Specify the optimizer with a lower learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Specify the evalutation metrics\n",
        "metrics = {'f1_score': f1_score, 'auroc': roc_auc_score}\n",
        "\n",
        "#Training the model\n",
        "# trained_model = train_model(model, criterion, dataloaders,\n",
        "#                             optimizer, bpath=\"\", metrics=metrics, num_epochs=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ldYsgbBP77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.save(trained_model, os.path.join('drive/Shared drives/Capstone Summer 2020/Models', 'weights.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X_KKctJHOl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_model = load_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH7vHJN74vzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "outputId": "01b96609-15ab-46e6-9c4b-9fb58ff4a619"
      },
      "source": [
        "# val_path = \"drive/Shared drives/Capstone Summer 2020/Data/TestImage/\"\n",
        "\n",
        "# from torch.utils.data import Dataset as DDD\n",
        "\n",
        "# val_dataset = DDD(val_path, )\n",
        "# val_dataloader =  DataLoader (val_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
        "\n",
        "imageName = \"V0001\"\n",
        "\n",
        "mask_path = \"drive/Shared drives/Capstone Summer 2020/Data/Validation/Masks/\"+ imageName +\".bmp\"\n",
        "ground_truth = Image.open(mask_path)\n",
        "\n",
        "img_path = \"drive/Shared drives/Capstone Summer 2020/Data/Validation/Images/\"+ imageName +\".jpg\"\n",
        "img_prediction = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "result_mask = predict(img_prediction, trained_model)\n",
        "\n",
        "print(result_mask)\n",
        "# mas = torch.argmax(result_mask.squeeze(), dim=1).detach().cpu().numpy()\n",
        "mas = result_mask.argmax(0).detach().cpu().numpy()\n",
        "print(mas.shape)\n",
        "# mas = torch.max(result_mask, 1)\n",
        "# out = result_mask.argmax(0)\n",
        "# print(out.size())\n",
        "\n",
        "# create a color pallette, selecting a color for each class\n",
        "# palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "# colors = torch.as_tensor([i for i in range(3)])[:, None] * palette\n",
        "# colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "\n",
        "# # plot the semantic segmentation predictions of 21 classes in each color\n",
        "# r = Image.fromarray(out.byte().cpu().numpy()).resize((512,512))\n",
        "# r.putpalette(colors)\n",
        "##\n",
        "\n",
        "# plt.imshow(r)\n",
        "\n",
        "# print('==' )\n",
        "from matplotlib import pyplot as plt\n",
        "fig, axs = plt.subplots(1, 2, figsize=(22,5))\n",
        "\n",
        "#plot ground truth\n",
        "axs[0].imshow((ground_truth))\n",
        "axs[0].axis('off')   \n",
        "axs[0].set_title('Ground truth')\n",
        "\n",
        "#plot the mask\n",
        "axs[1].imshow((mas))\n",
        "axs[1].axis('off')   \n",
        "axs[1].set_title('Mask')\n",
        "\n",
        "# for row in result_mask:\n",
        "#   # print(row)\n",
        "#   if 0 not in row:\n",
        "    # print(\"yayay\")\n",
        "# print(mas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512])\n",
            "tensor([[0.9811, 0.9811, 0.9811,  ..., 0.9853, 0.9853, 0.9853],\n",
            "        [0.9811, 0.9811, 0.9811,  ..., 0.9853, 0.9853, 0.9853],\n",
            "        [0.9811, 0.9811, 0.9811,  ..., 0.9853, 0.9853, 0.9853],\n",
            "        ...,\n",
            "        [0.9629, 0.9629, 0.9629,  ..., 0.9853, 0.9853, 0.9853],\n",
            "        [0.9629, 0.9629, 0.9629,  ..., 0.9853, 0.9853, 0.9853],\n",
            "        [0.9629, 0.9629, 0.9629,  ..., 0.9853, 0.9853, 0.9853]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "(512,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6f0bfa100c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#plot the mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (512,) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1584x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqazAITNc6kK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "31e6715a-6096-4de8-94db-37d36049ddc6"
      },
      "source": [
        "a = torch.randn(4, 4)\n",
        "a\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1663, -1.0868,  0.2287, -1.2824],\n",
              "        [-1.3855, -0.8027, -0.3674, -0.6257],\n",
              "        [ 0.2527,  2.4022,  0.2912, -0.8184],\n",
              "        [ 0.6920, -0.7488,  0.9105, -0.2727]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1m9T4pLc-FX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dc85885-1081-4de0-cc0f-d80c1a3c64ac"
      },
      "source": [
        "torch.argmax(a, dim=0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 2, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn3p7opNdNBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d6cdf3d-824b-47b4-b2a3-0df58b97c7bd"
      },
      "source": [
        "torch.argmax(a, dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffcrz3etliAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "tranny = transforms.Compose([ToPILImage()])\n",
        "what = tranny(train_dataset[0])\n",
        "print(what['image'])\n",
        "im_running_out_of_names = what['image']\n",
        "im_running_out_of_names.show()\n",
        "display(im_running_out_of_names)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}